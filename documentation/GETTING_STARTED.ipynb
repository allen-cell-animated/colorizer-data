{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started: how to prepare data for Timelapse Feature Explorer\n",
        "\n",
        "The [Timelapse Feature Explorer (TFE)](https://timelapse.allencell.org) is a web-based application designed for the interactive visualization and analysis of segmented time-series microscopy data! Data needs to be processed into a specific format to be loaded into the viewer.\n",
        "\n",
        "In this tutorial, you'll learn how to prepare your data for the Timelapse Feature Explorer.\n",
        "\n",
        "*This notebook can be run with Google Colab, but we recommend running it on a local machine if you plan to convert and view your own datasets later.*\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/allen-cell-animated/colorizer-data/blob/doc/getting-started-guide/documentation/GETTING_STARTED.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "## 1. Terms\n",
        "\n",
        "A few key terms:\n",
        "\n",
        "- **Dataset**: A dataset is a single time-series containing tracked objects and features. A dataset can contain up to about 16.7 million unique objects.\n",
        "- **Raw dataset**: The raw data that you have collected or generated, before processing into the TFE format.\n",
        "- **Collection**: An arbitrary grouping of datasets, allowing for easy comparison of multiple datasets in the viewer.\n",
        "- **Object ID**: An ID associated with a single segmentation at a single timepoint. In the TFE-accepted format, object IDs must be sequential, starting from 0, and be unique across the whole dataset.\n",
        "- **Track ID**: An identifier for a unique set of objects, linking their object IDs across timepoints. Generally this describes the track of one object along the time sequence.\n",
        "\n",
        "## 2. Prerequisites\n",
        "\n",
        "### Installing `colorizer-data` and tutorial dependencies\n",
        "\n",
        "From a command terminal, clone this repository and run the following commands to install dependencies. This will install the necessary libraries for the example scripts and the latest release of `colorizer-data`. (You may want to do this from a virtual Python environment-- see [venv](https://docs.python.org/3/library/venv.html) or [conda](https://docs.conda.io/en/latest/) for more information.)\n",
        "\n",
        "> **_NOTE_**: You must be on Python version 3.9 or above. The installation may fail unexpectedly on older versions of Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skip this step if you've already cloned the repository.\n",
        "!git clone https://github.com/allen-cell-animated/colorizer-data.git\n",
        "%cd colorizer-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up and activate a virtual environment. You should be inside `colorizer-data`.\n",
        "!python -m venv venv\n",
        "!source venv/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd colorizer-data/documentation/getting_started_guide\n",
        "%pip install -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Expected formats for raw data\n",
        "\n",
        "For this tutorial, we'll be working with sample data included in the [`getting_started_guide/raw_dataset`](./getting_started_guide/raw_dataset/) directory.\n",
        "\n",
        "This dataset is a simplified example of raw, pre-processed segmentation data. The data was generated using the [`generate_raw_data.py` script](./getting_started_guide/scripts/generate_data.py), which generates a **CSV file** with columns for object IDs, track IDs, times, centroids, features (volume/height), and paths to the segmentation images. The **segmentation images** are 2D images in the OME-TIFF format, encoding the locations of segmented objects.\n",
        "\n",
        "If your segmentation images are 3D, you may choose to flatten them to 2D or use our provided utilites to do so.\n",
        "\n",
        "Your tracked object data may be in a different format, in which case it will need to be transformed to work well with our utilities. Generally, we recommend:\n",
        "\n",
        "1. Save your data as a CSV or other format that can be read into a pandas `DataFrame`.\n",
        "2. Make every segmented object its own row in the table.\n",
        "3. Save track ID, time, centroids, and other information as columns.\n",
        "4. Create columns for any additional features you want to visualize.\n",
        "\n",
        "### What does the example dataset look like?\n",
        "\n",
        "Here's a preview of the raw dataset, `data.csv`:\n",
        "\n",
        "| object_id | track_id | time | centroid_x | centroid_y | area | radius | location | segmentation_path |\n",
        "| ----------- | ---------- | ------ | ------------ | ------------ | -------- | -------- | ------------------- | --- |\n",
        "| 1 | 0 | 0 | 33 | 110 | 706.9 | 15 | middle | frame_0.tiff |\n",
        "| 2 | 1 | 0 | 67 | 100 | 804.2 | 16 | middle | frame_0.tiff |\n",
        "| 3 | 2 | 0 | 100 | 108 | 804.2 | 16 | middle | frame_0.tiff |\n",
        "| 4 | 3 | 0 | 133 | 88 | 706.9 | 15 | middle | frame_0.tiff |\n",
        "| 5 | 4 | 0 | 167 | 101 | 804.2 | 16 | middle | frame_0.tiff |\n",
        "| 6 | 0 | 1 | 33 | 121 | 530.9 | 13 | bottom | frame_1.tiff |\n",
        "| 7 | 1 | 1 | 67 | 113 | 804.2 | 16 | middle | frame_1.tiff |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "\n",
        "Each of the segmentation images in the `segmentation_path` column is an OME-TIFF image containing the IDs of the segmented objects:\n",
        "\n",
        "_Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing._\n",
        "\n",
        "![Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing. The black background has been labeled as ID=0. Five red bubbles in the center of the image are labeled in ascending order from 1-5, left to right.](getting_started_guide\\assets\\sample-segmentation.png)\n",
        "\n",
        "> **_NOTE:_** A value of `0` is used to represent the background in the segmentation images. For simplicity, we recommend starting object IDs at `1` to avoid conflicts with the background value.\n",
        "\n",
        "## 4. Processing data\n",
        "\n",
        "Timelapse Feature Explorer reads data in the format specified by the [`DATA_FORMAT`](./DATA_FORMAT.md) document. We'll use the utilities provided by `colorizer-data` to convert to this format.\n",
        "\n",
        "### Processing script\n",
        "\n",
        "#### 1. Import dependencies and load the dataset into a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from bioio import BioImage\n",
        "import pandas as pd\n",
        "\n",
        "from colorizer_data.utils import (\n",
        "    remap_segmented_image,\n",
        ")\n",
        "from colorizer_data.writer import (\n",
        "    ColorizerDatasetWriter,\n",
        "    ColorizerMetadata,\n",
        "    FeatureInfo,\n",
        "    FeatureType,\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "source_dataset_directory = Path(\"raw_dataset\")\n",
        "data: pd.DataFrame = pd.read_csv(source_dataset_directory / \"data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Configure the writer and data columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column names as they appear in the loaded csv. Some of these are core data timelapse and others are the extra calculated features for use in the viewer.\n",
        "\n",
        "# Core data columns\n",
        "OBJECT_ID_COLUMN = \"object_id\"\n",
        "TRACK_ID_COLUMN = \"track_id\"\n",
        "TIMES_COLUMN = \"time\"\n",
        "SEGMENTED_IMAGE_COLUMN = \"segmentation_path\"\n",
        "CENTROIDS_X_COLUMN = \"centroid_x\"\n",
        "CENTROIDS_Y_COLUMN = \"centroid_y\"\n",
        "\n",
        "# Feature columns\n",
        "AREA_COLUMN = \"area\"\n",
        "LOCATION_COLUMN = \"location\"\n",
        "RADIUS_COLUMN = \"radius\"\n",
        "\n",
        "# Add in a column to act as an index for the dataset.\n",
        "# This preserves row numbers even when the dataframe is grouped by\n",
        "# time later.\n",
        "INDEX_COLUMN = \"index\"\n",
        "data = data.reset_index(drop=True)\n",
        "data[INDEX_COLUMN] = data.index.values\n",
        "\n",
        "# Create the writer, which we'll be using to save dataset-related files.\n",
        "output_dir = \".\"\n",
        "dataset_name = \"processed_dataset\"\n",
        "writer = ColorizerDatasetWriter(output_dir, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Write the core data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Turn core data columns into a numpy array, to be saved by the writer.\n",
        "tracks = data[TRACK_ID_COLUMN].to_numpy()\n",
        "times = data[TIMES_COLUMN].to_numpy()\n",
        "centroids_x = data[CENTROIDS_X_COLUMN].to_numpy()\n",
        "centroids_y = data[CENTROIDS_Y_COLUMN].to_numpy()\n",
        "\n",
        "writer.write_data(\n",
        "    tracks=tracks,\n",
        "    times=times,\n",
        "    centroids_x=centroids_x,\n",
        "    centroids_y=centroids_y,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Write the features\n",
        "\n",
        "Features can be one of three types:\n",
        "\n",
        "1. **Continuous** features are used for floating-point numbers.\n",
        "2. **Discrete** features are used for integers.\n",
        "3. **Categorical** features are used for string-based labels. Note that there can only be a max of 12 categories for a categorical feature.\n",
        "\n",
        "The `FeatureInfo` class is used to provide metadata about each feature, such as its label, key, type, and units if applicable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "areas = data[AREA_COLUMN].to_numpy()\n",
        "locations = data[LOCATION_COLUMN].to_numpy()\n",
        "radii = data[RADIUS_COLUMN].to_numpy()\n",
        "\n",
        "# Additional metadata can be provided for each feature, which will be shown\n",
        "# when interacting with it in the viewer.\n",
        "area_info = FeatureInfo(\n",
        "    label=\"Area\",\n",
        "    key=\"area\",\n",
        "    type=FeatureType.CONTINUOUS,\n",
        "    unit=\"px²\",\n",
        ")\n",
        "radius_info = FeatureInfo(\n",
        "    label=\"Radius\",\n",
        "    key=\"radius\",\n",
        "    # Discrete features are used for integers.\n",
        "    type=FeatureType.DISCRETE,\n",
        "    unit=\"px\",\n",
        ")\n",
        "location_info = FeatureInfo(\n",
        "    label=\"Location\",\n",
        "    key=\"location\",\n",
        "    # Categorical features are used for string-based labels.\n",
        "    type=FeatureType.CATEGORICAL,\n",
        "    # Categories can be auto-detected from the data, or provided manually\n",
        "    # if you want to preserve a specific order for the labels.\n",
        "    categories=[\"top\", \"middle\", \"bottom\"],\n",
        ")\n",
        "writer.write_feature(areas, area_info)\n",
        "writer.write_feature(radii, radius_info)\n",
        "writer.write_feature(locations, location_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Write the images\n",
        "\n",
        "The `ColorizerDatasetWriter` needs to prepare images for fast visualization by converting them to PNGs with encoded object IDs. You can see more about what this looks like and how it works in our [data format documentation](./DATA_FORMAT.md#5-frames). As previously noted, all object IDs must be unique too, so this next section will perform three tasks:\n",
        "\n",
        "1. Load in the image data from the segmentation images\n",
        "2. Remap the object IDs to be unique across all timepoints\n",
        "3. Write the remapped images using `ColorizerDatasetWriter.write_image()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group data by the timestamp\n",
        "data_grouped_by_time = data.groupby(TIMES_COLUMN)\n",
        "frame_paths = []\n",
        "\n",
        "for frame_num, frame_data in data_grouped_by_time:\n",
        "    # Get the path to the segmentation image and load it.\n",
        "    frame_path = frame_data.iloc[0][SEGMENTED_IMAGE_COLUMN]\n",
        "    segmentation_image = BioImage(source_dataset_directory / frame_path).get_image_data(\n",
        "        \"YX\", S=0, T=0, C=0\n",
        "    )\n",
        "    # NOTE: For datasets with 3D segmentations, you may need to flatten the data into\n",
        "    # 2D images. Typically, it's simplest to do so with a max projection, but may vary\n",
        "    # based on your data. Replace the above line with the following:\n",
        "    #\n",
        "    # segmentation_image = bioio.BioImage(frame_path).get_image_data(\"ZYX\", S=0, T=0, C=0)\n",
        "    # segmentation_image = segmentation_image.max(axis=0)\n",
        "    #\n",
        "    # Remap the segmented so object IDs are unique across all timepoints.\n",
        "    (remapped_segmentations, _lut) = remap_segmented_image(\n",
        "        segmentation_image, frame_data, OBJECT_ID_COLUMN, INDEX_COLUMN\n",
        "    )\n",
        "    # Write the new segmentation image.\n",
        "    frame_prefix = \"frame_\"\n",
        "    frame_suffix = \".png\"\n",
        "    writer.write_image(remapped_segmentations, frame_num, frame_prefix, frame_suffix)\n",
        "    image_path = frame_prefix + str(frame_num) + frame_suffix\n",
        "    frame_paths.append(image_path)\n",
        "\n",
        "writer.set_frame_paths(frame_paths)\n",
        "print(frame_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. Write the dataset and any additional metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the metadata for this dataset.\n",
        "metadata = ColorizerMetadata(\n",
        "    name=\"Example dataset\",\n",
        "    description=\"An example dataset for the Timelapse Feature Explorer.\",\n",
        "    author=\"Jane Doe et al.\",\n",
        "    dataset_version=\"v1.0\",\n",
        "    # The width and height of the original segmentations, in any arbitrary units.\n",
        "    # This will control the scale bar in the viewer.\n",
        "    frame_width=100,\n",
        "    frame_height=100,\n",
        "    frame_units=\"nm\",\n",
        "    # Time elapsed between each frame capture, in seconds.\n",
        "    frame_duration_sec=1,\n",
        ")\n",
        "\n",
        "# Write the final dataset\n",
        "writer.write_manifest(metadata=metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! If you got this far, you have a complete dataset ready to view in the TFE viewer. The new dataset will be found in the `processed_dataset` directory.\n",
        "\n",
        "## 5. Viewing the dataset\n",
        "\n",
        "Now that the dataset is processed, we can view it in the Timelapse Feature Explorer!\n",
        "\n",
        "Timelapse Feature Explorer is designed to load datasets using HTTP(S). Your data must be hosted on a **web server** and served over **HTTPS** to be loaded on our public build. Alternatively, you can run a **local web server** and a **local instance** of the viewer. We'll cover steps for both options.\n",
        "\n",
        "### 5.1 Viewing datasets via the web\n",
        "\n",
        "For this tutorial, you can load a pre-processed example copy of the dataset, which we've hosted on GitHub. You can access it at this URL: `https://raw.githubusercontent.com/allen-cell-animated/colorizer-data/doc/getting-started-guide/documentation/getting_started_guide/example/processed_dataset/manifest.json`\n",
        "\n",
        "If you have updated the dataset files or want to use a local dataset, skip to the next section.\n",
        "\n",
        "\n",
        "#### Opening your dataset with data on a web server (HTTPS)\n",
        "\n",
        "1. Open Timelapse Feature Explorer at [https://timelapse.allencell.org](https://timelapse.allencell.org).\n",
        "\n",
        "2. Click the **Load** in the header and paste in the following URL: `https://raw.githubusercontent.com/allen-cell-animated/colorizer-data/doc/getting-started-guide/documentation/getting_started_guide/example/processed_dataset/manifest.json`\n",
        "\n",
        "![The Load button on the Timelapse Feature Explorer header, next to the Help dropdown.](./getting_started_guide/assets/load-button.png)\n",
        "\n",
        "> **_NOTE:_** You can either provide the URL of the directory containing a `manifest.json` or the full URL path of a `.json` file that follows the [manifest specification](./DATA_FORMAT.md#dataset). We recommend specifying the full URL path that includes the `manifest.json`.\n",
        "\n",
        "Click **Load** in the popup menu to load the dataset. The viewer should appear with the dataset loaded!\n",
        "\n",
        "### 5.2 Opening your dataset from a local directory over HTTP\n",
        "\n",
        "To view a **locally converted dataset**, we'll also need to run a **local version** of the Timelapse Feature Explorer.\n",
        "\n",
        "<details>\n",
        "<summary><b>[Why do we need to run a local instance of the viewer?]</b></summary>\n",
        "\n",
        "---\n",
        "The public version of TFE is served over HTTPS, which is a secure protocol. (The \"S\" stands for secure!) For security reasons, HTTPS pages cannot load HTTP content, which is enforced by the browser. This means that the public version of TFE can only access other web content also hosted on HTTPS.\n",
        "\n",
        "Configuring an HTTPS server is complicated and requires a certificate. Instead, if we run TFE locally on HTTP, we can run a local server to also serve our local files over HTTP.\n",
        "---\n",
        "</details>\n",
        "\n",
        "\n",
        "> ⚠ If you've been following along in Google Colab, this is the point where you'll want to switch to your local machine. Rerun the previous steps locally or download the processed dataset into `colorizer-data/documentation/getting_started_guide/processed_dataset` and continue from here.\n",
        "\n",
        "#### 1. TFE installation\n",
        "\n",
        "Install Node (v20 or above) from [nodejs.org](https://nodejs.org/). Once Node is installed, **open a command terminal** wherever you want to install TFE. Run the following commands to clone the repository and install the dependencies:\n",
        "\n",
        "```bash\n",
        "# Check that node is installed\n",
        "node --version\n",
        "\n",
        "# In a different directory than colorizer-data:\n",
        "git clone https://github.com/allen-cell-animated/timelapse-colorizer.git\n",
        "cd timelapse-colorizer\n",
        "npm install\n",
        "npm run start\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now run `npm run start` at anytime in this directory to start the viewer. By default, it will be mounted at `http://localhost:5173`.\n",
        "\n",
        "#### 2. Serving local files\n",
        "\n",
        "We've provided a simple script to start a local server and serve the processed dataset files. Open a command terminal in the `getting_started_guide` directory and run the following command:\n",
        "```bash\n",
        "python scripts/run_local_server.py 8080\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running the notebook, you can also run this code block:\n",
        "!python scripts/run_local_server.py 8080"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. View the dataset\n",
        "\n",
        "Once both steps are done, open your web browser and navigate to [`http://localhost:5173`](http://localhost:5173). Click the **Load** button in the header and paste in the following URL: `http://localhost:8080/processed_dataset`.\n",
        "\n",
        "Your dataset should appear in the browser and be ready for viewing!\n",
        "\n",
        "![The loaded dataset in Timelapse Feature Explorer. Five bubbles appear in the main viewport in various shades of purple and blue. The selected feature is Area, in units of pixels squared.](./getting_started_guide/assets/loaded-dataset.png)\n",
        "\n",
        "Click on the features dropdown to select a feature to visualize. You can also hover on individual objects to see their feature values.\n",
        "\n",
        "## 6. What's next?\n",
        "\n",
        "### Collections\n",
        "\n",
        "If you have multiple datasets that you want to group together, you can create a **collection**. When loaded in the viewer, collections allows you to switch between any of the included datasets for easier comparison. See the [section on collections in our data format documentation](./DATA_FORMAT.md#collections) for more information.\n",
        "\n",
        "Also see the `update_collection()` method in the [`utils.py` file](../colorizer_data/utils.py) to create and update collections.\n",
        "\n",
        "<details>\n",
        "<summary><b>[🔍 Show me an example!]</b></summary>\n",
        "\n",
        "---\n",
        "\n",
        "Let's say we have two datasets that we want to group together in a collection. The directory structure would look like this:\n",
        "\n",
        "```txt\n",
        "📄 collection.json\n",
        "📂 dataset_1/\n",
        "  - 📄 manifest.json\n",
        "  ...\n",
        "📂 dataset_2/\n",
        "  - 📄 manifest.json\n",
        "  - ...\n",
        "```\n",
        "\n",
        "The `collection.json` file would look something like this:\n",
        "\n",
        "`manifest.json:`\n",
        "\n",
        "```txt\n",
        "{\n",
        "    \"datasets\": [\n",
        "        { \"name\": \"Dataset 1\", \"path\": \"dataset_1\" },\n",
        "        { \"name\": \"Dataset 2\", \"path\": \"dataset_2\" },\n",
        "    ],\n",
        "    \"metadata\": {\n",
        "        \"name\": \"My custom collection\",\n",
        "        \"description\": \"Two example datasets grouped together for comparison\",\n",
        "        \"author\": \"Jane Doe\",\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "### Advanced dataset conversion\n",
        "\n",
        "We provide a number of example scripts that perform more advanced data processing tasks, such as:\n",
        "\n",
        "1. Including bounding box data and outliers\n",
        "2. Handling 3D segmentation images\n",
        "3. Parallelizing frame processing\n",
        "4. Grouping datasets into collections\n",
        "5. Handling common arguments/options for processing scripts\n",
        "\n",
        "All of these scripts are available in the [`bin/example_scripts` directory](../colorizer_data/bin/example_scripts/).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
