{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: how to prepare data for Timelapse Feature Explorer\n",
    "\n",
    "The [Timelapse Feature Explorer (TFE)](https://timelapse.allencell.org) is a web-based application designed for the interactive visualization and analysis of segmented time-series microscopy data! Data needs to be processed into a specific format to be loaded into the viewer.\n",
    "\n",
    "In this tutorial, you'll learn how to prepare your data for the Timelapse Feature Explorer.\n",
    "\n",
    "*This notebook can be run with Google Colab, but we recommend running it on a local machine if you plan to convert and view your own datasets later.*\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/allen-cell-animated/colorizer-data/blob/main/documentation/getting_started_guide/GETTING_STARTED.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "Setup instructions differ based on whether you are running a local Jupyter notebook instance or Google Colab.\n",
    "> **_NOTE_**: You must be on Python version 3.9 or above. The installation may fail unexpectedly on older versions of Python.\n",
    "\n",
    "#### 1.1A Running on a local machine with **Jupyter Lab**\n",
    "\n",
    "From a command terminal, clone this repository if you haven't already and run Jupyter Lab from the repository root directory. \n",
    "You will likely want to do this from a virtual Python environment. We've included steps below on how to activate a [venv](https://docs.python.org/3/library/venv.html) virtual Python environment.\n",
    "\n",
    "```bash\n",
    "# Skip this step if you've already cloned the repository.\n",
    "git clone https://github.com/allen-cell-animated/colorizer-data.git\n",
    "cd colorizer-data\n",
    "\n",
    "# Set up a virtual environment\n",
    "python -m venv venv\n",
    "source .venv/bin/activate\n",
    "# On Windows, you may need to run this instead:\n",
    "# source .venv/Scripts/activate\n",
    "\n",
    "# Install and start Jupyter lab\n",
    "python -m pip install jupyterlab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Follow the provided link to open the browser to access Jupyter, and navigate to this notebook in Jupyter Lab. Run the following cell to set up and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1B Running in **Google Colab**\n",
    "\n",
    "If opening this notebook in Google Colab, run the following commands to install the repository and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/allen-cell-animated/colorizer-data.git\n",
    "\n",
    "%cd colorizer-data/documentation/getting_started_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terminology\n",
    "\n",
    "- **Segmentation ID**: An ID associated with a single segmentation shape at a single timepoint. This is also commonly also called \"label\" in segmentation & tracking workflows.\n",
    "- **Track ID**: An identifier for a unique set of segmentations, linking them across timepoints. Generally this describes the track of one object along the time sequence. \n",
    "\n",
    "## 3. Expected formats for raw data\n",
    "\n",
    "For this tutorial, we'll be working with sample data included in the [`getting_started_guide/raw_datasets`](./raw_datasets/) directory.\n",
    "\n",
    "This dataset is a simplified example of raw, pre-processed segmentation data. The data was generated using the [`generate_raw_data.py` script](./scripts/generate_data.py), which generates a **CSV file** with columns for segmentation IDs, track IDs, times, centroids, features (volume/height), and paths to the segmentation images. \n",
    "\n",
    "The **segmentation images** are 2D images in the OME-TIFF format, encoding the locations of segmented objects. Pixel values correspond to **segmentation IDs** , where 0 is the background and positive integers correspond to different segmented objects. If your segmentation images are 3-dimensional, you may choose to flatten them to 2D or use our provided utilites to do so. We also offer experimental 3D support in Timelapse Feature Explorer for OME-Zarr array data.\n",
    "\n",
    "Your data may be in a different format, in which case it will need to be transformed to work well with our utilities. Generally, we recommend the following steps:\n",
    "\n",
    "1. Save your data as a CSV or other format that can be read into a pandas `DataFrame`.\n",
    "2. Make every segmentation at each time point into its own row in the table.\n",
    "3. Save track ID, time, centroids, and other information as columns.\n",
    "\n",
    "### What does the example dataset look like?\n",
    "\n",
    "Here's a preview of the raw dataset, `data.csv`:\n",
    "\n",
    "| segmentation_id | track_id | time | centroid_x | centroid_y | area | radius | location | segmentation_path |\n",
    "| ----------- | ---------- | ------ | ------------ | ------------ | -------- | -------- | ------------------- | --- |\n",
    "| 1 | 0 | 0 | 33 | 110 | 706.9 | 15 | middle | frame_0.tiff |\n",
    "| 2 | 1 | 0 | 67 | 100 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 3 | 2 | 0 | 100 | 108 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 4 | 3 | 0 | 133 | 88 | 706.9 | 15 | middle | frame_0.tiff |\n",
    "| 5 | 4 | 0 | 167 | 101 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 1 | 0 | 1 | 33 | 121 | 530.9 | 13 | bottom | frame_1.tiff |\n",
    "| 2 | 1 | 1 | 67 | 113 | 804.2 | 16 | middle | frame_1.tiff |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "Each of the segmentation images in the `segmentation_path` column is an OME-TIFF image with segmentation IDs encoded as pixel values:\n",
    "\n",
    "_Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing._\n",
    "\n",
    "![Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing. The black background has been labeled as ID=0. Five red bubbles in the center of the image are labeled in ascending order from 1-5, left to right.](./assets/sample-segmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing data\n",
    "\n",
    "Timelapse Feature Explorer reads data in the format specified by the [`DATA_FORMAT` document](../DATA_FORMAT.md). We'll use the utilities provided by `colorizer-data` to convert to this format.\n",
    "\n",
    "### 4.1. Basic usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from colorizer_data import convert_colorizer_data\n",
    "import pandas as pd\n",
    "\n",
    "source_directory = Path(\"raw_datasets/dataset_1\")\n",
    "output_directory = Path(\"processed_dataset\")\n",
    "\n",
    "# Load the dataset\n",
    "data: pd.DataFrame = pd.read_csv(source_directory / \"data.csv\")\n",
    "\n",
    "# `convert_colorizer_data` is a helper function for dataset conversion! You can pass in a pandas DataFrame\n",
    "# and the output directory, as well as the names of the data columns for the id, time, track, and image data.\n",
    "# Any other columns are treated as features. We'll show off more advanced behavior in later steps, but this will\n",
    "# output a complete readable dataset.\n",
    "\n",
    "convert_colorizer_data(\n",
    "    data,\n",
    "    output_directory,\n",
    "    source_dir=source_directory,\n",
    "    object_id_column=\"segmentation_id\",\n",
    "    times_column=\"time\",\n",
    "    track_column=\"track_id\",\n",
    "    image_column=\"segmentation_path\",\n",
    "    centroid_x_column=\"centroid_x\",\n",
    "    centroid_y_column=\"centroid_y\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a complete dataset ready to view in the TFE viewer. The new dataset will be found in the `processed_dataset` directory. You can skip to [section 5, 'Viewing the dataset'](#5-Viewing-the-dataset) to view your dataset in the TFE viewer, or run a few additional steps for a more polished dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Advanced usage\n",
    "\n",
    "#### Feature metadata\n",
    "\n",
    "By default, any columns that aren't mapped to data columns (e.g. object ID, times, tracks, etc.) are automatically handled as feature data. You can specify which columns are features by passing a list of column names to the `feature_columns` parameter.\n",
    "\n",
    "While `convert_colorizer_data` will infer feature types for you, you can also provide metadata about each feature, such as its label, key, type, units, and description using the `feature_info` argument.\n",
    "\n",
    "Features can be one of three types:\n",
    "\n",
    "1. **Continuous** features are used for floating-point numbers.\n",
    "2. **Discrete** features are used for integers.\n",
    "3. **Categorical** features are used for string-based labels. (Note that there is a hard limit of 12 categories for a categorical feature.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorizer_data import (\n",
    "    FeatureInfo,\n",
    "    FeatureType,\n",
    ")\n",
    "\n",
    "area_info = FeatureInfo(\n",
    "    label=\"Area\",\n",
    "    key=\"area\",\n",
    "    type=FeatureType.CONTINUOUS,\n",
    "    unit=\"pxÂ²\",\n",
    "    description=\"Area of object in square pixels, calculated from radius.\"\n",
    ")\n",
    "radius_info = FeatureInfo(\n",
    "    label=\"Radius\",\n",
    "    key=\"radius\",\n",
    "    # Discrete features are used for integers.\n",
    "    type=FeatureType.DISCRETE,\n",
    "    unit=\"px\",\n",
    "    description=\"Radius of object in pixels.\"\n",
    ")\n",
    "location_info = FeatureInfo(\n",
    "    label=\"Location\",\n",
    "    key=\"location\",\n",
    "    # Categorical features are used for string-based labels.\n",
    "    type=FeatureType.CATEGORICAL,\n",
    "    # Categories can be auto-detected from the data, or provided manually\n",
    "    # if you want to preserve a specific order for the labels.\n",
    "    categories=[\"top\", \"middle\", \"bottom\"],\n",
    "    description=\"Y position of object's centroid in the frame, as either 'top' (y < 40%), 'middle' (40% â‰¤ y â‰¤ 60%), or 'bottom' (y > 60%) of the frame.\"\n",
    ")\n",
    "\n",
    "# Map from column names to FeatureInfo objects.\n",
    "feature_info = {\n",
    "    \"area\": area_info,\n",
    "    \"radius\": radius_info,\n",
    "    \"location\": location_info\n",
    "}\n",
    "# Note that providing `feature_column_names` will turn off automatic feature detection.\n",
    "# Providing it is optional, and `feature_info` will still update metadata without it.\n",
    "feature_column_names = [\"area\", \"radius\", \"location\"]\n",
    "\n",
    "convert_colorizer_data(\n",
    "    data,\n",
    "    output_directory,\n",
    "    source_dir=source_directory,\n",
    "    object_id_column=\"segmentation_id\",\n",
    "    times_column=\"time\",\n",
    "    track_column=\"track_id\",\n",
    "    image_column=\"segmentation_path\",\n",
    "    centroid_x_column=\"centroid_x\",\n",
    "    centroid_y_column=\"centroid_y\",\n",
    "    feature_column_names=feature_column_names,\n",
    "    feature_info=feature_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset metadata\n",
    "\n",
    "We recommend including additional metadata about your dataset, which can be provided using the `metadata` argument. This includes the dataset name, author, description, and any additional information you want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorizer_data import (\n",
    "    ColorizerMetadata,\n",
    ")\n",
    "\n",
    "# Define the metadata for this dataset.\n",
    "metadata = ColorizerMetadata(\n",
    "    name=\"Example dataset 1\",\n",
    "    description=\"An example dataset for the Timelapse Feature Explorer!\",\n",
    "    author=\"Jane Doe et al.\",\n",
    "    dataset_version=\"v1.0\",\n",
    "    # The width and height of the original segmentations, in units defined\n",
    "    # by `frame_units`. This configures the scale bar in the viewer.\n",
    "    frame_width=100,\n",
    "    frame_height=100,\n",
    "    frame_units=\"nm\",\n",
    "    # Time elapsed between each frame capture, in seconds.\n",
    "    frame_duration_sec=1,\n",
    ")\n",
    "\n",
    "convert_colorizer_data(\n",
    "    data,\n",
    "    output_directory,\n",
    "    source_dir=source_directory,\n",
    "    object_id_column=\"segmentation_id\",\n",
    "    times_column=\"time\",\n",
    "    track_column=\"track_id\",\n",
    "    image_column=\"segmentation_path\",\n",
    "    centroid_x_column=\"centroid_x\",\n",
    "    centroid_y_column=\"centroid_y\",\n",
    "    feature_column_names=feature_column_names,\n",
    "    feature_info=feature_info,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collections (optional)\n",
    "\n",
    "If you have multiple datasets, you can group them into a **collection** to make it easier to view and compare them in Timelapse Feature Explorer. These steps are optional but will let you quickly switch between two different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the existing dataset to a subdirectory\n",
    "import os\n",
    "os.makedirs(\"processed_dataset/dataset_1\", exist_ok=True)\n",
    "!mv processed_dataset/*.parquet processed_dataset/dataset_1/\n",
    "!mv processed_dataset/*.png processed_dataset/dataset_1/\n",
    "!mv processed_dataset/*.json processed_dataset/dataset_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an additional dataset for the example.\n",
    "source_directory = Path(\"raw_datasets/dataset_2\")\n",
    "output_directory = Path(\"processed_dataset/dataset_2\")\n",
    "data: pd.DataFrame = pd.read_csv(source_directory / \"data.csv\")\n",
    "\n",
    "convert_colorizer_data(\n",
    "    data,\n",
    "    output_directory,\n",
    "    source_dir=source_directory,\n",
    "    object_id_column=\"segmentation_id\",\n",
    "    times_column=\"time\",\n",
    "    track_column=\"track_id\",\n",
    "    image_column=\"segmentation_path\",\n",
    "    centroid_x_column=\"centroid_x\",\n",
    "    centroid_y_column=\"centroid_y\",\n",
    "    feature_column_names=feature_column_names,\n",
    "    feature_info=feature_info,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collections are represented with a JSON file containing a list of paths to datasets and their display names. The `update_collection` utility can be used to create or update a collection file.\n",
    "\n",
    "Right now, our datasets are in subdirectories within the `processed_dataset` directory. We'll place our collection file at the top level of the `processed_dataset` directory, alongside our datasets. The collection file will store relative paths to our two datasets.\n",
    "\n",
    "The final directory structure should look like this:\n",
    "```txt\n",
    "ðŸ“ processed_dataset\n",
    "â”œâ”€â”€ ðŸ“„ collection.json\n",
    "â”œâ”€â”€ ðŸ“ dataset_1\n",
    "|   â””â”€â”€ ...\n",
    "â””â”€â”€ ðŸ“ dataset_2\n",
    "    â””â”€â”€ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorizer_data import update_collection, CollectionMetadata\n",
    "\n",
    "# Like with datasets, collections can also include optional metadata.\n",
    "collection_metadata = CollectionMetadata(\n",
    "    name=\"Example collection\",\n",
    "    description=\"An example collection of datasets for the Timelapse Feature Explorer!\",\n",
    "    author=\"Jane Doe et al.\",\n",
    "    collection_version=\"v1.0\",\n",
    ")\n",
    "\n",
    "# Create the collection file and add our two datasets.\n",
    "# update_collection(collection_path, display name, relative_dataset_path, optional metadata)\n",
    "update_collection(\n",
    "    \"processed_dataset/collection.json\", \"Dataset 1\", \"dataset_1/manifest.json\", metadata=collection_metadata\n",
    ")\n",
    "update_collection(\n",
    "    \"processed_dataset/collection.json\", \"Dataset 2\", \"dataset_2/manifest.json\", metadata=collection_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Viewing the dataset\n",
    "\n",
    "![The loaded dataset in Timelapse Feature Explorer. Five bubbles appear in the main viewport in various shades of purple and blue. The selected feature is Area, in units of pixels squared.](./assets/loaded-dataset.png)\n",
    "\n",
    "Now that the dataset is processed, we can view it in the Timelapse Feature Explorer!\n",
    "\n",
    "There are several ways to load datasets into the viewer:\n",
    "\n",
    "1. Load via CLI\n",
    "2. Load from a zip file\n",
    "3. Load via the web (HTTPS)\n",
    "\n",
    "We recommend hosting your datasets on an HTTPS-accessible server (option 3) whenever possible, because it makes sharing and collaboration simpler. We'll include instructions for all three methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Open locally via CLI\n",
    "\n",
    "We've provided a simple command-line script to open a local dataset in TFE, `tfe-open`. It launches a local instance of the viewer and serves the dataset over HTTP.\n",
    "\n",
    "> âš  The `tfe-open` script will not work in **Google Colab**, so this step can only be run on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the project has been installed (see README instructions, `pip install -e '.[dev]'`),\n",
    "# you can run `tfe-open` directly from the command line:\n",
    "# !tfe-open ./processed_dataset\n",
    "\n",
    "# For this example, we'll run the tool directly from the Python file.\n",
    "# Make sure Jupyter Lab is running from the root of the repository directory.\n",
    "!python ../../colorizer_data/bin/tfe_open.py ./processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The viewer should open in a new browser tab with the dataset loaded. \n",
    "\n",
    "To stop the server, return to the terminal and press `Ctrl+C` or close the\n",
    "terminal window. (On Jupyter, press the stop button on the code block.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load zip file (for Google Colab)\n",
    "\n",
    "If running on Google Colab, you can load a dataset from a `.zip` file containing the processed dataset files.\n",
    "\n",
    "1. Create and download the zip file of the processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE COLAB:\n",
    "!zip -r processed_dataset.zip processed_dataset\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"processed_dataset.zip\")\n",
    "\n",
    "# If the download doesn't start (can be browser-dependent), you can manually\n",
    "# navigate to and download the zip file from the file browser on the left side\n",
    "# of the screen. It will be in\n",
    "# `colorizer_data/documentation/getting_started_guide/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LOCAL: LINUX + MACOS\n",
    "!zip -r processed_dataset.zip processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF LOCAL: WINDOWS\n",
    "# On Windows, the most reliable way to get a valid zip is to manually locate\n",
    "# and right click to compress the dataset folder to a zip file. `tar.exe` may\n",
    "# not be present on older versions of Windows.\n",
    "!C:/Windows/System32/tar.exe -caf processed_dataset.zip processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Open Timelapse Feature Explorer: [https://timelapse.allencell.org](https://timelapse.allencell.org).\n",
    "3. Click **Load** in the top-right corner, then select **Load .zip file**.\n",
    "4. Select the zip file.\n",
    "\n",
    "You should now be able to view and explore your dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Viewing datasets via the web (HTTPS)\n",
    "\n",
    "Timelapse Feature Explorer can load datasets over the web using any URL starting with `https://`. This is the recommended way to interact with datasets, because it makes sharing and collaboration simpler.\n",
    "\n",
    "There are several options for hosting files online. A few include:\n",
    "- [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html)\n",
    "- [Google Cloud Storage bucket](https://cloud.google.com/storage?hl=en)\n",
    "- GitHub repository (not recommended as GitHub may impose rate limits)\n",
    "\n",
    "For this example, you can load a dataset from our GitHub Repository:\n",
    "\n",
    "1. Open Timelapse Feature Explorer at [https://timelapse.allencell.org](https://timelapse.allencell.org).\n",
    "2. Click the **Load** button in the header.\n",
    "3. Paste in the following URL: `https://raw.githubusercontent.com/allen-cell-animated/colorizer-data/main/documentation/getting_started_guide/example/processed_dataset/`\n",
    "4. Click **Load**.\n",
    "\n",
    "The viewer should appear with the dataset loaded!\n",
    "\n",
    "For HTTPS-hosted datasets, you can share your current view in TFE by clicking the **Share** button in the top right corner to get a shareable link. This will allow anyone else with access to the dataset URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What's next?\n",
    "\n",
    "### Advanced conversion\n",
    "\n",
    "[`convert_colorizer_data`](https://github.com/allen-cell-animated/colorizer-data/blob/e3fb0520a823f0c15396937c8b385f6de7798401/colorizer_data/converter.py#L390) includes detailed documentation for additional configuration options and parameters.\n",
    "\n",
    "This includes support for:\n",
    "- **Outliers**, objects that should have unique visualization (e.g. dead cells)\n",
    "- **Backdrop images**, comparison images (like fluorescence channels) to display with segmentations\n",
    "- **3D segmentation data**, using OME-Zarr for fast volumetric visualization.\n",
    "\n",
    "For example, for 3D data, you can specify a 3D frame source like this:\n",
    "\n",
    "```python\n",
    "from colorizer_data import convert_colorizer_data\n",
    "from colorizer_data.types import Frames3dMetadata\n",
    "\n",
    "frames_3d = Frames3dMetadata(\n",
    "    # Can be a relative path or a URL\n",
    "    source=\"https://some-bucket.com/image.ome.zarr\",\n",
    "    segmentation_channel=0,\n",
    "    total_frames=50,\n",
    ")\n",
    "\n",
    "convert_colorizer_data(\n",
    "    ...\n",
    "    frames_3d=frames_3d,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "If you need more control over the dataset conversion process, we include several examples of advanced usage in the [`bin/example_scripts` directory](https://github.com/allen-cell-animated/colorizer-data/tree/main/documentation/bin/example_scripts).\n",
    "\n",
    "### Installing Timelapse Feature Explorer locally\n",
    "The pre-built version of TFE may not be up to date with the latest features and bugfixes. If you want to run your own local instance of TFE, you can clone the repository yourself and run it locally.\n",
    "\n",
    "The repository for TFE and instructions on installation can be found at [https://github.com/allen-cell-animated/timelapse-colorizer](https://github.com/allen-cell-animated/timelapse-colorizer).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
