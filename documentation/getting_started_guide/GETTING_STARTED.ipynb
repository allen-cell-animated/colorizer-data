{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: how to prepare data for Timelapse Feature Explorer\n",
    "\n",
    "The [Timelapse Feature Explorer (TFE)](https://timelapse.allencell.org) is a web-based application designed for the interactive visualization and analysis of segmented time-series microscopy data! Data needs to be processed into a specific format to be loaded into the viewer.\n",
    "\n",
    "In this tutorial, you'll learn how to prepare your data for the Timelapse Feature Explorer.\n",
    "\n",
    "*This notebook can be run with Google Colab, but we recommend running it on a local machine if you plan to convert and view your own datasets later.*\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/allen-cell-animated/colorizer-data/blob/main/documentation/getting_started_guide/GETTING_STARTED.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "</a>\n",
    "\n",
    "## 1. Terms\n",
    "\n",
    "A few key terms:\n",
    "\n",
    "- **Dataset**: A dataset is a single time-series containing tracked objects and features. A dataset can contain up to about 16.7 million unique objects.\n",
    "- **Raw dataset**: The raw data that you have collected or generated, before processing into the TFE format.\n",
    "- **Collection**: An arbitrary grouping of datasets, allowing for easy comparison of multiple datasets in the viewer.\n",
    "- **Object ID**: An ID associated with a single segmentation at a single timepoint. In the TFE-accepted format, object IDs must be sequential, starting from 0, and be unique across the whole dataset.\n",
    "- **Track ID**: An identifier for a unique set of objects, linking their object IDs across timepoints. Generally this describes the track of one object along the time sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites\n",
    "\n",
    "Setup instructions differ based on whether you are running a local Jupyter notebook instance or Google Colab.\n",
    "> **_NOTE_**: You must be on Python version 3.9 or above. The installation may fail unexpectedly on older versions of Python.\n",
    "\n",
    "#### 2.1A Running on a local machine with **Jupyter Lab**\n",
    "\n",
    "From a command terminal, clone this repository if you haven't already and navigate to the `getting-started-guide` directory.\n",
    "You will likely want to do this from a virtual Python environment. We've included steps below on how to activate a [venv](https://docs.python.org/3/library/venv.html) virtual Python environment.\n",
    "\n",
    "```bash\n",
    "# Skip this step if you've already cloned the repository.\n",
    "git clone https://github.com/allen-cell-animated/colorizer-data.git\n",
    "cd colorizer-data\n",
    "\n",
    "# Set up a virtual environment\n",
    "python -m venv venv\n",
    "source .venv/bin/activate\n",
    "# On Windows, you may need to run this instead:\n",
    "# source .venv/Scripts/activate\n",
    "\n",
    "# Install and start Jupyter lab\n",
    "python -m pip install jupyterlab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Follow the provided link to open the browser to access Jupyter, and navigate to this notebook in Jupyter Lab. Run the following cell to set up and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes.\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1B Running in **Google Colab**\n",
    "\n",
    "If opening this notebook in Google Colab, run the following commands to install the repository and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/allen-cell-animated/colorizer-data.git\n",
    "\n",
    "%cd colorizer-data/documentation/getting_started_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expected formats for raw data\n",
    "\n",
    "For this tutorial, we'll be working with sample data included in the [`getting_started_guide/raw_dataset`](./raw_dataset/) directory.\n",
    "\n",
    "This dataset is a simplified example of raw, pre-processed segmentation data. The data was generated using the [`generate_raw_data.py` script](./scripts/generate_data.py), which generates a **CSV file** with columns for object IDs, track IDs, times, centroids, features (volume/height), and paths to the segmentation images. The **segmentation images** are 2D images in the OME-TIFF format, encoding the locations of segmented objects.\n",
    "\n",
    "If your segmentation images are 3D, you may choose to flatten them to 2D or use our provided utilites to do so.\n",
    "\n",
    "Your tracked object data may be in a different format, in which case it will need to be transformed to work well with our utilities. Generally, we recommend:\n",
    "\n",
    "1. Save your data as a CSV or other format that can be read into a pandas `DataFrame`.\n",
    "2. Make every segmented object its own row in the table.\n",
    "3. Save track ID, time, centroids, and other information as columns.\n",
    "4. Create columns for any additional features you want to visualize.\n",
    "\n",
    "### What does the example dataset look like?\n",
    "\n",
    "Here's a preview of the raw dataset, `data.csv`:\n",
    "\n",
    "| object_id | track_id | time | centroid_x | centroid_y | area | radius | location | segmentation_path |\n",
    "| ----------- | ---------- | ------ | ------------ | ------------ | -------- | -------- | ------------------- | --- |\n",
    "| 1 | 0 | 0 | 33 | 110 | 706.9 | 15 | middle | frame_0.tiff |\n",
    "| 2 | 1 | 0 | 67 | 100 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 3 | 2 | 0 | 100 | 108 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 4 | 3 | 0 | 133 | 88 | 706.9 | 15 | middle | frame_0.tiff |\n",
    "| 5 | 4 | 0 | 167 | 101 | 804.2 | 16 | middle | frame_0.tiff |\n",
    "| 6 | 0 | 1 | 33 | 121 | 530.9 | 13 | bottom | frame_1.tiff |\n",
    "| 7 | 1 | 1 | 67 | 113 | 804.2 | 16 | middle | frame_1.tiff |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "Each of the segmentation images in the `segmentation_path` column is an OME-TIFF image containing the IDs of the segmented objects:\n",
    "\n",
    "_Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing._\n",
    "\n",
    "![Frame 0 of the example dataset, as viewed in FIJI. Contrast has been increased for easier viewing. The black background has been labeled as ID=0. Five red bubbles in the center of the image are labeled in ascending order from 1-5, left to right.](./assets/sample-segmentation.png)\n",
    "\n",
    "> **_NOTE:_** A value of `0` is used to represent the background in the segmentation images. For simplicity, we recommend starting object IDs at `1` to avoid conflicts with the background value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing data\n",
    "\n",
    "Timelapse Feature Explorer reads data in the format specified by the [`DATA_FORMAT`](../DATA_FORMAT.md) document. We'll use the utilities provided by `colorizer-data` to convert to this format.\n",
    "\n",
    "### Processing script\n",
    "\n",
    "#### 1. Import dependencies and load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from bioio import BioImage\n",
    "import pandas as pd\n",
    "\n",
    "from colorizer_data.utils import (\n",
    "    remap_segmented_image,\n",
    ")\n",
    "from colorizer_data.writer import (\n",
    "    ColorizerDatasetWriter,\n",
    "    ColorizerMetadata,\n",
    "    FeatureInfo,\n",
    "    FeatureType,\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "source_dataset_directory = Path(\"raw_dataset\")\n",
    "data: pd.DataFrame = pd.read_csv(source_dataset_directory / \"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Configure the writer and data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names as they appear in the loaded csv. Some of these are core data timelapse and others are the extra calculated features for use in the viewer.\n",
    "\n",
    "# Core data columns\n",
    "OBJECT_ID_COLUMN = \"object_id\"\n",
    "TRACK_ID_COLUMN = \"track_id\"\n",
    "TIMES_COLUMN = \"time\"\n",
    "SEGMENTED_IMAGE_COLUMN = \"segmentation_path\"\n",
    "CENTROIDS_X_COLUMN = \"centroid_x\"\n",
    "CENTROIDS_Y_COLUMN = \"centroid_y\"\n",
    "\n",
    "# Feature columns\n",
    "AREA_COLUMN = \"area\"\n",
    "LOCATION_COLUMN = \"location\"\n",
    "RADIUS_COLUMN = \"radius\"\n",
    "\n",
    "# Add in a column to act as an index for the dataset.\n",
    "# This preserves row numbers even when the dataframe is grouped by\n",
    "# time later.\n",
    "INDEX_COLUMN = \"index\"\n",
    "data = data.reset_index(drop=True)\n",
    "data[INDEX_COLUMN] = data.index.values\n",
    "\n",
    "# Create the writer, which we'll be using to save dataset-related files.\n",
    "output_dir = \".\"\n",
    "dataset_name = \"processed_dataset\"\n",
    "writer = ColorizerDatasetWriter(output_dir, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Write the core data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn core data columns into a numpy array, to be saved by the writer.\n",
    "tracks = data[TRACK_ID_COLUMN].to_numpy()\n",
    "times = data[TIMES_COLUMN].to_numpy()\n",
    "centroids_x = data[CENTROIDS_X_COLUMN].to_numpy()\n",
    "centroids_y = data[CENTROIDS_Y_COLUMN].to_numpy()\n",
    "\n",
    "writer.write_data(\n",
    "    tracks=tracks,\n",
    "    times=times,\n",
    "    centroids_x=centroids_x,\n",
    "    centroids_y=centroids_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Write the features\n",
    "\n",
    "Features can be one of three types:\n",
    "\n",
    "1. **Continuous** features are used for floating-point numbers.\n",
    "2. **Discrete** features are used for integers.\n",
    "3. **Categorical** features are used for string-based labels. Note that there can only be a max of 12 categories for a categorical feature.\n",
    "\n",
    "The `FeatureInfo` class is used to provide metadata about each feature, such as its label, key, type, units, and descriptions if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = data[AREA_COLUMN].to_numpy()\n",
    "locations = data[LOCATION_COLUMN].to_numpy()\n",
    "radii = data[RADIUS_COLUMN].to_numpy()\n",
    "\n",
    "# Additional metadata can be provided for each feature, which will be shown\n",
    "# when interacting with it in the viewer.\n",
    "area_info = FeatureInfo(\n",
    "    label=\"Area\",\n",
    "    key=\"area\",\n",
    "    type=FeatureType.CONTINUOUS,\n",
    "    unit=\"px²\",\n",
    "    description=\"Area of object in square pixels, calculated from radius.\"\n",
    ")\n",
    "radius_info = FeatureInfo(\n",
    "    label=\"Radius\",\n",
    "    key=\"radius\",\n",
    "    # Discrete features are used for integers.\n",
    "    type=FeatureType.DISCRETE,\n",
    "    unit=\"px\",\n",
    "    description=\"Radius of object in pixels.\"\n",
    ")\n",
    "location_info = FeatureInfo(\n",
    "    label=\"Location\",\n",
    "    key=\"location\",\n",
    "    # Categorical features are used for string-based labels.\n",
    "    type=FeatureType.CATEGORICAL,\n",
    "    # Categories can be auto-detected from the data, or provided manually\n",
    "    # if you want to preserve a specific order for the labels.\n",
    "    categories=[\"top\", \"middle\", \"bottom\"],\n",
    "    description=\"Y position of object's centroid in the frame, as either 'top' (y < 40%), 'middle' (40% ≤ y ≤ 60%), or 'bottom' (y > 60%) of the frame.\"\n",
    ")\n",
    "writer.write_feature(areas, area_info)\n",
    "writer.write_feature(radii, radius_info)\n",
    "writer.write_feature(locations, location_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Write the images\n",
    "\n",
    "The `ColorizerDatasetWriter` needs to prepare images for fast visualization by converting them to PNGs with encoded object IDs. You can see more about what this looks like and how it works in our [data format documentation](../DATA_FORMAT.md#5-frames). As previously noted, all object IDs must be unique too, so this next section will perform three tasks:\n",
    "\n",
    "1. Load in the image data from the segmentation images\n",
    "2. Remap the object IDs to be unique across all timepoints\n",
    "3. Write the remapped images using `ColorizerDatasetWriter.write_image()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by the timestamp\n",
    "data_grouped_by_time = data.groupby(TIMES_COLUMN)\n",
    "frame_paths = []\n",
    "\n",
    "for frame_num, frame_data in data_grouped_by_time:\n",
    "    # Get the path to the segmentation image and load it.\n",
    "    frame_path = frame_data.iloc[0][SEGMENTED_IMAGE_COLUMN]\n",
    "    segmentation_image = BioImage(source_dataset_directory / frame_path).get_image_data(\n",
    "        \"YX\", S=0, T=0, C=0\n",
    "    )\n",
    "    # NOTE: For datasets with 3D segmentations, you may need to flatten the data into\n",
    "    # 2D images. Typically, it's simplest to do so with a max projection, but may vary\n",
    "    # based on your data. Replace the above line with the following:\n",
    "    #\n",
    "    # segmentation_image = bioio.BioImage(frame_path).get_image_data(\"ZYX\", S=0, T=0, C=0)\n",
    "    # segmentation_image = segmentation_image.max(axis=0)\n",
    "    #\n",
    "    # Remap the segmented so object IDs are unique across all timepoints.\n",
    "    (remapped_segmentations, _lut) = remap_segmented_image(\n",
    "        segmentation_image, frame_data, OBJECT_ID_COLUMN, INDEX_COLUMN\n",
    "    )\n",
    "    # Write the new segmentation image.\n",
    "    frame_prefix = \"frame_\"\n",
    "    frame_suffix = \".png\"\n",
    "    writer.write_image(remapped_segmentations, frame_num, frame_prefix, frame_suffix)\n",
    "    image_path = frame_prefix + str(frame_num) + frame_suffix\n",
    "    frame_paths.append(image_path)\n",
    "\n",
    "writer.set_frame_paths(frame_paths)\n",
    "print(frame_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Write the dataset and any additional metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metadata for this dataset.\n",
    "metadata = ColorizerMetadata(\n",
    "    name=\"Example dataset\",\n",
    "    description=\"An example dataset for the Timelapse Feature Explorer.\",\n",
    "    author=\"Jane Doe et al.\",\n",
    "    dataset_version=\"v1.0\",\n",
    "    # The width and height of the original segmentations, in any arbitrary units.\n",
    "    # This will control the scale bar in the viewer.\n",
    "    frame_width=100,\n",
    "    frame_height=100,\n",
    "    frame_units=\"nm\",\n",
    "    # Time elapsed between each frame capture, in seconds.\n",
    "    frame_duration_sec=1,\n",
    ")\n",
    "\n",
    "# Write the final dataset\n",
    "writer.write_manifest(metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! If you got this far, you have a complete dataset ready to view in the TFE viewer. The new dataset will be found in the `processed_dataset` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Viewing the dataset\n",
    "\n",
    "Now that the dataset is processed, we can view it in the Timelapse Feature Explorer!\n",
    "\n",
    "Timelapse Feature Explorer is designed to load datasets over the web using HTTP(S). This makes it easier to share datasets with collaborators and the public.\n",
    "\n",
    "Your data must be hosted on a **web server** and served over **HTTPS** to be loaded on our public build. Alternatively, you can run a **local web server** and a **local instance** of the viewer. We'll cover steps for both options.\n",
    "\n",
    "### 5.1 Viewing datasets via the web (HTTPS)\n",
    "\n",
    "For this tutorial, you can load a pre-processed example copy of the dataset, which we've hosted on GitHub. You can access it at this URL: `https://raw.githubusercontent.com/allen-cell-animated/colorizer-data/main/documentation/getting_started_guide/example/processed_dataset/manifest.json`\n",
    "\n",
    "If you have updated the dataset files or want to use a local dataset, skip to the next section.\n",
    "\n",
    "1. Open Timelapse Feature Explorer at [https://timelapse.allencell.org](https://timelapse.allencell.org).\n",
    "\n",
    "2. Click the **Load** in the header and paste in the following URL: `https://raw.githubusercontent.com/allen-cell-animated/colorizer-data/main/documentation/getting_started_guide/example/processed_dataset/manifest.json`\n",
    "\n",
    "![The Load button on the Timelapse Feature Explorer header, next to the Help dropdown.](./assets/load-button.png)\n",
    "\n",
    "> **_NOTE:_** You can either provide the URL of the directory containing a `manifest.json` or the full URL path of a `.json` file that follows the [manifest specification](../DATA_FORMAT.md#dataset). We recommend specifying the full URL path that includes the `manifest.json`.\n",
    "\n",
    "Click **Load** in the popup menu to load the dataset. The viewer should appear with the dataset loaded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Opening your dataset from a local directory over HTTP\n",
    "\n",
    "To view a **locally converted dataset**, we'll also need to run a **local version** of the Timelapse Feature Explorer.\n",
    "\n",
    "> ⚠ If you've been following along in Google Colab, this is the point where you'll want to switch to your local machine. Rerun the previous steps locally or download the processed dataset into `colorizer-data/documentation/getting_started_guide/processed_dataset` and continue from here.\n",
    "\n",
    "<details>\n",
    "<summary><b>[Why do we need to run a local instance of the viewer?]</b></summary>\n",
    "\n",
    "--- \n",
    "\n",
    "TFE only loads data over the HTTP(S) web protocols, rather than directly loading local files. We can get around this by running a local server to serve our local files over HTTP.\n",
    "\n",
    "However, our public build of TFE is served over HTTPS, a secure version of the HTTP protocol. For security reasons, HTTPS pages cannot load HTTP content. This means that the public version of TFE can only access other web content also hosted on HTTPS. \n",
    "\n",
    "It's difficult to set up an HTTPS server, so instead we can run TFE locally on HTTP! This will let it also access our local HTTP server.\n",
    "\n",
    "![A diagram. The public version of TFE (HTTPS) can load files from a remote file server (HTTPS) but can't load it from a local file server (HTTP). A local version of TFE (HTTP) can load from both remote file servers and local file servers (HTTP and HTTPS).](./assets/http-diagram.png)\n",
    "\n",
    "---\n",
    "</details>\n",
    "\n",
    "We've provided a simple script to start a local server and serve the processed dataset files. Open a command terminal in the `getting_started_guide` directory and run the following command:\n",
    "\n",
    "```bash\n",
    "python scripts/run_local_server.py 8080\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running this notebook locally (not in Colab), you can also run this code block to start the local server.\n",
    "# Click stop in the toolbar to stop the server.\n",
    "!python scripts/run_local_server.py 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open your web browser and navigate to [`http://localhost:8080`](http://localhost:8080). You'll see a directory of files.\n",
    "\n",
    "![A server webpage, titled \"Directory listing for /\". A list of files and directories follows. The \"processed_dataset/\" and \"viewer/\" directories are highlighted.](./assets/directory.png)\n",
    "\n",
    "Navigate to the `processed_dataset` directory and right click on the **`manifest.json`** file. Select **Copy link address** to copy the URL of the file.\n",
    "\n",
    "![A list of files as blue hyperlinks. The \"manifest.json\" file is highlighted and the context menu has appeared. The \"Copy link address\" option is hovered.](./assets/save-manifest.png)\n",
    "\n",
    "Then, go back to the root directory of the file server and click the **`viewer/`** directory to open the viewer. Go to the **Load** button and paste in the copied URL of the `manifest.json` file to load the dataset.\n",
    "\n",
    "![The Load button on the Timelapse Feature Explorer header, next to the Help dropdown.](./assets/load-button.png)\n",
    "\n",
    "Once the dataset is open, click on the features dropdown to select a feature to visualize. You can also hover on individual objects to see their feature values in the viewport.\n",
    "\n",
    "![The loaded dataset in Timelapse Feature Explorer. Five bubbles appear in the main viewport in various shades of purple and blue. The selected feature is Area, in units of pixels squared.](./assets/loaded-dataset.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What's next?\n",
    "\n",
    "### Collections\n",
    "\n",
    "If you have multiple datasets that you want to group together, you can create a **collection**. When loaded in the viewer, collections allows you to switch between any of the included datasets for easier comparison. See the [section on collections in our data format documentation](../DATA_FORMAT.md#collections) for more information.\n",
    "\n",
    "Also see the `update_collection()` method in the [`utils.py` file](../../colorizer_data/utils.py) to create and update collections.\n",
    "\n",
    "<details>\n",
    "<summary><b>[🔍 Show me an example!]</b></summary>\n",
    "\n",
    "---\n",
    "\n",
    "Let's say we have two datasets that we want to group together in a collection. The directory structure would look like this:\n",
    "\n",
    "```txt\n",
    "📄 collection.json\n",
    "📂 dataset_1/\n",
    "  - 📄 manifest.json\n",
    "  ...\n",
    "📂 dataset_2/\n",
    "  - 📄 manifest.json\n",
    "  - ...\n",
    "```\n",
    "\n",
    "The `collection.json` file would look something like this:\n",
    "\n",
    "`manifest.json:`\n",
    "\n",
    "```txt\n",
    "{\n",
    "    \"datasets\": [\n",
    "        { \"name\": \"Dataset 1\", \"path\": \"dataset_1\" },\n",
    "        { \"name\": \"Dataset 2\", \"path\": \"dataset_2\" },\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"name\": \"My custom collection\",\n",
    "        \"description\": \"Two example datasets grouped together for comparison\",\n",
    "        \"author\": \"Jane Doe\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "\n",
    "### Advanced dataset conversion\n",
    "\n",
    "We provide a number of example scripts that perform more advanced data processing tasks, such as:\n",
    "\n",
    "1. Including bounding box data and outliers\n",
    "2. Handling 3D segmentation images\n",
    "3. Parallelizing frame processing\n",
    "4. Grouping datasets into collections\n",
    "5. Handling common arguments/options for processing scripts\n",
    "\n",
    "All of these scripts are available in the [`bin/example_scripts` directory](../../colorizer_data/bin/example_scripts/).\n",
    "\n",
    "### Installing Timelapse Feature Explorer locally\n",
    "The pre-built version of TFE may not be up to date with the latest features and bugfixes. If you want to run your own local instance of TFE, you can clone the repository yourself and run it locally.\n",
    "\n",
    "The repository for TFE and instructions on installation can be found at [https://github.com/allen-cell-animated/timelapse-colorizer](https://github.com/allen-cell-animated/timelapse-colorizer).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
